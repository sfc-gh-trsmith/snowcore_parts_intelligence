{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "title_and_objectives"
   },
   "source": [
    "# Part Similarity Analysis Using Text Embeddings\n",
    "\n",
    "## Business Objective\n",
    "\n",
    "Identify similar parts across the parts catalog by analyzing textual descriptions. This enables:\n",
    "- **Consolidation opportunities**: Find duplicate or near-duplicate parts for inventory reduction\n",
    "- **Substitution recommendations**: Suggest alternatives when a part is unavailable\n",
    "- **Procurement optimization**: Group similar parts for bulk purchasing\n",
    "\n",
    "## Technical Approach\n",
    "\n",
    "We use **text embeddings** (dense vector representations) generated by Snowflake Cortex's E5-base-v2 model to capture semantic meaning from part names, descriptions, and materials. Similarity between parts is computed via **cosine similarity** on their embedding vectors.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will understand:\n",
    "1. How text embeddings encode semantic meaning into numerical vectors\n",
    "2. Why cosine similarity is appropriate for comparing high-dimensional embeddings\n",
    "3. How to use Snowflake Cortex functions for ML-powered text analysis\n",
    "4. Best practices for fail-fast query execution in production notebooks\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Mathematics**: Basic linear algebra (vectors, dot products, norms)\n",
    "- **ML Concepts**: Understanding of embeddings and similarity metrics\n",
    "- **Python**: Familiarity with pandas DataFrames and matplotlib\n",
    "- **Domain**: General understanding of parts/inventory management\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "| Section | Purpose |\n",
    "|---------|----------|\n",
    "| 1. Title & Objectives | Frame the problem and learning goals |\n",
    "| 2. Environment Setup | Configure runtime, imports, and helpers |\n",
    "| 3. Data Loading | Load part master data with validation |\n",
    "| 4. Data Exploration | Analyze distributions and data quality |\n",
    "| 5. Embedding Generation | Create vector representations of parts |\n",
    "| 6. Similarity Computation | Calculate pairwise cosine similarity |\n",
    "| 7. Evaluation | Analyze and visualize similarity results |\n",
    "| 8. Production Output | Write results with verification |\n",
    "| 9. Key Takeaways | Summary, limitations, and next steps |\n",
    "\n",
    "## Output\n",
    "\n",
    "- **`DATA_SCIENCE.PART_EMBEDDINGS`**: 768-dimensional embedding vectors for each part\n",
    "- **`DATA_SCIENCE.PART_SIMILARITY_SCORES`**: Top-3 most similar parts for each source part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "environment_setup_header"
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Environment Setup\n",
    "\n",
    "This section configures the runtime environment including:\n",
    "- Package imports for data manipulation and visualization\n",
    "- Snowflake session initialization\n",
    "- Dark theme visualization settings (Snowflake-inspired, colorblind-safe)\n",
    "- Fail-fast query execution helper for production reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "# Standard library\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Snowflake\n",
    "from snowflake.snowpark.context import get_active_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "configure_visualization_theme"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION THEME: Snowflake-inspired dark mode\n",
    "# =============================================================================\n",
    "# Soft dark background (#121212) reduces eye strain vs pure black.\n",
    "# Off-white text (#E5E5E7) reduces glare vs pure white.\n",
    "# Colorblind-safe palette avoids red/green distinctions.\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams.update({\n",
    "    # Background colors (soft dark gray)\n",
    "    'figure.facecolor': '#121212',\n",
    "    'axes.facecolor': '#121212',\n",
    "    \n",
    "    # Text colors (off-white)\n",
    "    'text.color': '#E5E5E7',\n",
    "    'axes.labelcolor': '#E5E5E7',\n",
    "    'xtick.color': '#A1A1A6',\n",
    "    'ytick.color': '#A1A1A6',\n",
    "    \n",
    "    # Grid and axes (subtle)\n",
    "    'axes.edgecolor': '#3A3A3C',\n",
    "    'grid.color': '#2C2C2E',\n",
    "    'grid.alpha': 0.6,\n",
    "    \n",
    "    # Figure quality\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 200,\n",
    "    'figure.figsize': (10, 6),\n",
    "    \n",
    "    # Typography\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "})\n",
    "\n",
    "# Colorblind-safe palette for dark backgrounds\n",
    "SNOWFLAKE_COLORS = ['#64D2FF', '#FF9F0A', '#5AC8FA', '#FFD60A', '#11567F']\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=SNOWFLAKE_COLORS)\n",
    "\n",
    "print(\"Visualization theme configured (dark mode, colorblind-safe)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "snowflake_session_setup"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SNOWFLAKE SESSION\n",
    "# =============================================================================\n",
    "# Get session from Snowflake Notebooks context (do not create new connection)\n",
    "\n",
    "session = get_active_session()\n",
    "print(f\"Session established\")\n",
    "print(f\"  Current database: {session.get_current_database()}\")\n",
    "print(f\"  Current schema: {session.get_current_schema()}\")\n",
    "print(f\"  Current warehouse: {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "execute_query_helper_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FAIL-FAST QUERY HELPER\n",
    "# =============================================================================\n",
    "# All SQL queries must raise exceptions on failure - never suppress errors.\n",
    "# This ensures the notebook fails immediately if data issues occur.\n",
    "\n",
    "def execute_query(query: str, name: str = \"query\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Execute a SQL query with fail-fast error handling.\n",
    "    \n",
    "    Args:\n",
    "        query: SQL query string to execute\n",
    "        name: Descriptive name for error messages\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with query results\n",
    "    \n",
    "    Raises:\n",
    "        RuntimeError: If query fails or returns None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = session.sql(query).to_pandas()\n",
    "        if result is None:\n",
    "            raise RuntimeError(f\"Query '{name}' returned None\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Query '{name}' failed: {e}\") from e\n",
    "\n",
    "\n",
    "def execute_statement(query: str, name: str = \"statement\") -> None:\n",
    "    \"\"\"\n",
    "    Execute a SQL statement (DDL/DML) with fail-fast error handling.\n",
    "    \n",
    "    Args:\n",
    "        query: SQL statement to execute\n",
    "        name: Descriptive name for error messages\n",
    "    \n",
    "    Raises:\n",
    "        RuntimeError: If statement fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        session.sql(query).collect()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Statement '{name}' failed: {e}\") from e\n",
    "\n",
    "\n",
    "print(\"Query helpers defined: execute_query(), execute_statement()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "data_loading_header"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Data Loading\n",
    "\n",
    "Load part master data from `ATOMIC.PART_MASTER` and validate that required columns exist and the dataset is not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "load_part_master_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD PART MASTER DATA\n",
    "# =============================================================================\n",
    "# We load a sample of parts for embedding generation.\n",
    "# The LIMIT clause controls processing scope for development/testing.\n",
    "\n",
    "SAMPLE_LIMIT = 5000  # Number of parts to process\n",
    "\n",
    "parts_df = execute_query(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        GLOBAL_ID,\n",
    "        PART_NAME,\n",
    "        PART_DESCRIPTION,\n",
    "        MATERIAL\n",
    "    FROM ATOMIC.PART_MASTER\n",
    "    LIMIT {SAMPLE_LIMIT}\n",
    "    \"\"\",\n",
    "    name=\"load_part_master\"\n",
    ")\n",
    "\n",
    "# Validate data was loaded\n",
    "if len(parts_df) == 0:\n",
    "    raise RuntimeError(\"ATOMIC.PART_MASTER is empty - cannot proceed\")\n",
    "\n",
    "# Validate required columns exist\n",
    "required_columns = ['GLOBAL_ID', 'PART_NAME', 'PART_DESCRIPTION', 'MATERIAL']\n",
    "missing = set(required_columns) - set(parts_df.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "print(f\"Loaded {len(parts_df):,} parts from ATOMIC.PART_MASTER\")\n",
    "print(f\"\\nColumns: {list(parts_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "data_exploration_header"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Data Exploration\n",
    "\n",
    "Before generating embeddings, we analyze the input data to understand:\n",
    "- Distribution of text lengths (affects embedding quality)\n",
    "- Missing value rates (null descriptions will produce poor embeddings)\n",
    "- Material distribution (categorical feature included in embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "data_quality_analysis_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA QUALITY ANALYSIS\n",
    "# =============================================================================\n",
    "# Check for missing values that could degrade embedding quality\n",
    "\n",
    "# Calculate missing value rates\n",
    "missing_counts = parts_df.isnull().sum()\n",
    "missing_pct = (missing_counts / len(parts_df) * 100).round(2)\n",
    "\n",
    "print(\"Missing Value Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "for col in required_columns:\n",
    "    count = missing_counts[col]\n",
    "    pct = missing_pct[col]\n",
    "    status = \"\" if pct == 0 else \" (may affect embedding quality)\"\n",
    "    print(f\"  {col}: {count:,} missing ({pct}%){status}\")\n",
    "\n",
    "# Calculate text lengths for non-null values\n",
    "parts_df['text_length'] = (\n",
    "    parts_df['PART_NAME'].fillna('').str.len() +\n",
    "    parts_df['PART_DESCRIPTION'].fillna('').str.len() +\n",
    "    parts_df['MATERIAL'].fillna('').str.len()\n",
    ")\n",
    "\n",
    "print(f\"\\nText Length Statistics (combined name + description + material):\")\n",
    "print(f\"  Min: {parts_df['text_length'].min()} characters\")\n",
    "print(f\"  Max: {parts_df['text_length'].max()} characters\")\n",
    "print(f\"  Mean: {parts_df['text_length'].mean():.1f} characters\")\n",
    "print(f\"  Median: {parts_df['text_length'].median():.1f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "exploration_visualization_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA EXPLORATION: Visualizations\n",
    "# =============================================================================\n",
    "# Understanding text length distribution helps interpret embedding quality.\n",
    "# Very short texts may lack semantic content; very long texts may be truncated.\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Text length distribution\n",
    "ax1 = axes[0]\n",
    "ax1.hist(parts_df['text_length'], bins=50, color='#64D2FF', edgecolor='#121212', alpha=0.8)\n",
    "ax1.axvline(x=parts_df['text_length'].median(), color='#FF9F0A', linestyle='--', \n",
    "            linewidth=2, label=f\"Median: {parts_df['text_length'].median():.0f}\")\n",
    "ax1.set_xlabel('Combined Text Length (characters)')\n",
    "ax1.set_ylabel('Number of Parts')\n",
    "ax1.set_title('Distribution of Part Text Lengths')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Top materials by count\n",
    "ax2 = axes[1]\n",
    "material_counts = parts_df['MATERIAL'].fillna('(Unknown)').value_counts().head(10)\n",
    "bars = ax2.barh(range(len(material_counts)), material_counts.values, color='#5AC8FA', alpha=0.8)\n",
    "ax2.set_yticks(range(len(material_counts)))\n",
    "ax2.set_yticklabels(material_counts.index)\n",
    "ax2.set_xlabel('Number of Parts')\n",
    "ax2.set_title('Top 10 Materials by Part Count')\n",
    "ax2.invert_yaxis()  # Highest at top\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/part_data_exploration.png', dpi=150, bbox_inches='tight', facecolor='#121212')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nUnique materials: {parts_df['MATERIAL'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "embedding_algorithm_explainer"
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Embedding Generation\n",
    "\n",
    "### What are Text Embeddings?\n",
    "\n",
    "**Text embeddings** are dense vector representations that encode semantic meaning of text into numerical form. Unlike sparse representations (e.g., one-hot encoding or TF-IDF), embeddings place semantically similar texts close together in vector space.\n",
    "\n",
    "### Why E5-base-v2?\n",
    "\n",
    "We use Snowflake Cortex's `e5-base-v2` model because:\n",
    "1. **Optimized for similarity search**: E5 models are trained with contrastive learning specifically for retrieval/similarity tasks\n",
    "2. **768-dimensional output**: Provides rich semantic representation without excessive dimensionality\n",
    "3. **Native Snowflake integration**: No data movement required; embeddings computed in-warehouse\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The embedding function maps text to a 768-dimensional unit vector:\n",
    "\n",
    "$$\\mathbf{e} = \\text{E5}(\\text{text}) \\in \\mathbb{R}^{768}, \\quad ||\\mathbf{e}|| = 1$$\n",
    "\n",
    "We concatenate part attributes before embedding to capture the full semantic context:\n",
    "\n",
    "$$\\text{input} = \\text{PART\\_NAME} \\oplus \\text{PART\\_DESCRIPTION} \\oplus \\text{MATERIAL}$$\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "PART_MASTER Table\n",
    "       |\n",
    "       v\n",
    "[CONCAT: name + description + material]\n",
    "       |\n",
    "       v\n",
    "[EMBED_TEXT_768: E5-base-v2 model]\n",
    "       |\n",
    "       v\n",
    "PART_EMBEDDINGS Table (GLOBAL_ID, EMBEDDING[768])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "generate_embeddings_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE PART EMBEDDINGS\n",
    "# =============================================================================\n",
    "# Create 768-dimensional embedding vectors for each part using Snowflake Cortex.\n",
    "# The embedding captures semantic meaning from the concatenated text fields.\n",
    "\n",
    "embedding_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE DATA_SCIENCE.PART_EMBEDDINGS AS\n",
    "SELECT\n",
    "    GLOBAL_ID,\n",
    "    SNOWFLAKE.CORTEX.EMBED_TEXT_768(\n",
    "        'e5-base-v2',\n",
    "        CONCAT(\n",
    "            COALESCE(PART_NAME, ''), ' ',\n",
    "            COALESCE(PART_DESCRIPTION, ''), ' ',\n",
    "            COALESCE(MATERIAL, '')\n",
    "        )\n",
    "    ) AS EMBEDDING  -- (768,) vector per part\n",
    "FROM ATOMIC.PART_MASTER\n",
    "LIMIT {SAMPLE_LIMIT}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Generating embeddings (this may take a few minutes)...\")\n",
    "execute_statement(embedding_sql, name=\"create_part_embeddings\")\n",
    "\n",
    "# Verify embeddings were created\n",
    "embedding_count_df = execute_query(\n",
    "    \"SELECT COUNT(*) AS CNT FROM DATA_SCIENCE.PART_EMBEDDINGS\",\n",
    "    name=\"verify_embeddings\"\n",
    ")\n",
    "embedding_count = int(embedding_count_df['CNT'].iloc[0])\n",
    "\n",
    "if embedding_count == 0:\n",
    "    raise RuntimeError(\"No embeddings were generated - check input data\")\n",
    "\n",
    "print(f\"Generated {embedding_count:,} part embeddings\")\n",
    "print(f\"  Table: DATA_SCIENCE.PART_EMBEDDINGS\")\n",
    "print(f\"  Dimensions: 768 per embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "similarity_algorithm_explainer"
   },
   "source": [
    "---\n",
    "\n",
    "## 6. Similarity Computation\n",
    "\n",
    "### What is Cosine Similarity?\n",
    "\n",
    "**Cosine similarity** measures the angle between two vectors, ignoring their magnitudes. For unit vectors (like E5 embeddings), it equals the dot product:\n",
    "\n",
    "$$\\text{similarity}(\\mathbf{a}, \\mathbf{b}) = \\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{||\\mathbf{a}|| \\cdot ||\\mathbf{b}||} = \\mathbf{a} \\cdot \\mathbf{b}$$\n",
    "\n",
    "### Why Cosine Similarity for Embeddings?\n",
    "\n",
    "1. **Scale invariant**: Text length doesn't bias similarity (unlike Euclidean distance)\n",
    "2. **Bounded output**: Values in [-1, 1], where 1 = identical, 0 = orthogonal, -1 = opposite\n",
    "3. **Computationally efficient**: Dot product is fast for dense vectors\n",
    "\n",
    "### Output Schema\n",
    "\n",
    "We store the **top-3 most similar parts** for each source part:\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| SOURCE_GLOBAL_ID | VARCHAR | The reference part |\n",
    "| TARGET_GLOBAL_ID | VARCHAR | A similar part |\n",
    "| SIMILARITY_SCORE | FLOAT | Cosine similarity (0-100 scale) |\n",
    "| MATCH_REASON | VARCHAR | Method used for matching |\n",
    "\n",
    "### Why Top-3?\n",
    "\n",
    "Storing all pairwise similarities would create NÂ² rows (25M rows for 5K parts). Top-3 provides actionable recommendations while keeping the output manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "compute_similarity_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPUTE PAIRWISE SIMILARITY SCORES\n",
    "# =============================================================================\n",
    "# For each part, find the top-3 most similar parts based on embedding cosine\n",
    "# similarity. We exclude self-matches (a.GLOBAL_ID <> b.GLOBAL_ID).\n",
    "\n",
    "TOP_K = 3  # Number of similar parts to keep per source\n",
    "\n",
    "# Truncate existing scores for idempotent execution\n",
    "execute_statement(\n",
    "    \"TRUNCATE TABLE IF EXISTS DATA_SCIENCE.PART_SIMILARITY_SCORES\",\n",
    "    name=\"truncate_similarity_scores\"\n",
    ")\n",
    "\n",
    "# Compute and insert similarity scores\n",
    "similarity_sql = f\"\"\"\n",
    "INSERT INTO DATA_SCIENCE.PART_SIMILARITY_SCORES \n",
    "    (SOURCE_GLOBAL_ID, TARGET_GLOBAL_ID, SIMILARITY_SCORE, MATCH_REASON)\n",
    "WITH pairs AS (\n",
    "    SELECT\n",
    "        a.GLOBAL_ID AS SOURCE_GLOBAL_ID,\n",
    "        b.GLOBAL_ID AS TARGET_GLOBAL_ID,\n",
    "        -- Cosine similarity scaled to 0-100 for readability\n",
    "        VECTOR_COSINE_SIMILARITY(a.EMBEDDING, b.EMBEDDING) * 100 AS SIMILARITY_SCORE,\n",
    "        'E5 embedding cosine similarity' AS MATCH_REASON\n",
    "    FROM DATA_SCIENCE.PART_EMBEDDINGS a\n",
    "    JOIN DATA_SCIENCE.PART_EMBEDDINGS b\n",
    "        ON a.GLOBAL_ID <> b.GLOBAL_ID  -- Exclude self-matches\n",
    ")\n",
    "SELECT *\n",
    "FROM pairs\n",
    "-- Keep only top-K matches per source part\n",
    "QUALIFY ROW_NUMBER() OVER (\n",
    "    PARTITION BY SOURCE_GLOBAL_ID \n",
    "    ORDER BY SIMILARITY_SCORE DESC\n",
    ") <= {TOP_K}\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Computing top-{TOP_K} similarity scores per part...\")\n",
    "execute_statement(similarity_sql, name=\"insert_similarity_scores\")\n",
    "print(\"Similarity computation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "evaluation_header"
   },
   "source": [
    "---\n",
    "\n",
    "## 7. Evaluation\n",
    "\n",
    "Analyze the distribution of similarity scores to understand:\n",
    "- How similar are the \"most similar\" parts? (Are embeddings discriminative?)\n",
    "- What score thresholds indicate strong vs weak matches?\n",
    "- Are there outliers or unexpected patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "load_similarity_results_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD SIMILARITY RESULTS FOR ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "scores_df = execute_query(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        SOURCE_GLOBAL_ID,\n",
    "        TARGET_GLOBAL_ID,\n",
    "        SIMILARITY_SCORE,\n",
    "        MATCH_REASON\n",
    "    FROM DATA_SCIENCE.PART_SIMILARITY_SCORES\n",
    "    \"\"\",\n",
    "    name=\"load_similarity_scores\"\n",
    ")\n",
    "\n",
    "if len(scores_df) == 0:\n",
    "    raise RuntimeError(\"No similarity scores found - check computation step\")\n",
    "\n",
    "print(f\"Loaded {len(scores_df):,} similarity score records\")\n",
    "print(f\"  Unique source parts: {scores_df['SOURCE_GLOBAL_ID'].nunique():,}\")\n",
    "print(f\"  Score range: {scores_df['SIMILARITY_SCORE'].min():.2f} to {scores_df['SIMILARITY_SCORE'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "similarity_distribution_visualization"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMILARITY SCORE DISTRIBUTION ANALYSIS\n",
    "# =============================================================================\n",
    "# Visualize the distribution of similarity scores to establish thresholds\n",
    "# and understand embedding discriminativeness.\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Define interpretation thresholds\n",
    "HIGH_THRESHOLD = 80  # Strong match\n",
    "MEDIUM_THRESHOLD = 60  # Moderate match\n",
    "\n",
    "# Plot 1: Overall distribution with thresholds\n",
    "ax1 = axes[0]\n",
    "ax1.hist(scores_df['SIMILARITY_SCORE'], bins=50, color='#64D2FF', edgecolor='#121212', alpha=0.8)\n",
    "ax1.axvline(x=HIGH_THRESHOLD, color='#FF9F0A', linestyle='--', linewidth=2, label=f'High ({HIGH_THRESHOLD}+)')\n",
    "ax1.axvline(x=MEDIUM_THRESHOLD, color='#FFD60A', linestyle='--', linewidth=2, label=f'Medium ({MEDIUM_THRESHOLD}+)')\n",
    "ax1.set_xlabel('Similarity Score (0-100)')\n",
    "ax1.set_ylabel('Number of Pairs')\n",
    "ax1.set_title('Distribution of All Similarity Scores')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Distribution by rank (1st, 2nd, 3rd most similar)\n",
    "ax2 = axes[1]\n",
    "scores_df['RANK'] = scores_df.groupby('SOURCE_GLOBAL_ID')['SIMILARITY_SCORE'].rank(ascending=False, method='first')\n",
    "colors = ['#64D2FF', '#FF9F0A', '#5AC8FA']\n",
    "for rank in [1, 2, 3]:\n",
    "    rank_scores = scores_df[scores_df['RANK'] == rank]['SIMILARITY_SCORE']\n",
    "    ax2.hist(rank_scores, bins=30, alpha=0.6, label=f'Rank {int(rank)}', color=colors[rank-1])\n",
    "ax2.set_xlabel('Similarity Score (0-100)')\n",
    "ax2.set_ylabel('Number of Pairs')\n",
    "ax2.set_title('Score Distribution by Similarity Rank')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Box plot by rank\n",
    "ax3 = axes[2]\n",
    "rank_data = [scores_df[scores_df['RANK'] == r]['SIMILARITY_SCORE'].values for r in [1, 2, 3]]\n",
    "bp = ax3.boxplot(rank_data, labels=['1st', '2nd', '3rd'], patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax3.set_xlabel('Similarity Rank')\n",
    "ax3.set_ylabel('Similarity Score (0-100)')\n",
    "ax3.set_title('Score Range by Rank')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/similarity_distribution.png', dpi=150, bbox_inches='tight', facecolor='#121212')\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "high_count = (scores_df['SIMILARITY_SCORE'] >= HIGH_THRESHOLD).sum()\n",
    "medium_count = ((scores_df['SIMILARITY_SCORE'] >= MEDIUM_THRESHOLD) & \n",
    "                (scores_df['SIMILARITY_SCORE'] < HIGH_THRESHOLD)).sum()\n",
    "low_count = (scores_df['SIMILARITY_SCORE'] < MEDIUM_THRESHOLD).sum()\n",
    "\n",
    "print(f\"\\nSimilarity Score Summary:\")\n",
    "print(f\"  High (>={HIGH_THRESHOLD}): {high_count:,} pairs ({high_count/len(scores_df)*100:.1f}%)\")\n",
    "print(f\"  Medium ({MEDIUM_THRESHOLD}-{HIGH_THRESHOLD}): {medium_count:,} pairs ({medium_count/len(scores_df)*100:.1f}%)\")\n",
    "print(f\"  Low (<{MEDIUM_THRESHOLD}): {low_count:,} pairs ({low_count/len(scores_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "sample_high_similarity_pairs_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAMPLE HIGH-SIMILARITY PAIRS\n",
    "# =============================================================================\n",
    "# Examine some of the highest-scoring pairs to validate embedding quality.\n",
    "\n",
    "top_pairs_df = execute_query(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        s.SOURCE_GLOBAL_ID,\n",
    "        p1.PART_NAME AS SOURCE_PART_NAME,\n",
    "        p1.MATERIAL AS SOURCE_MATERIAL,\n",
    "        s.TARGET_GLOBAL_ID,\n",
    "        p2.PART_NAME AS TARGET_PART_NAME,\n",
    "        p2.MATERIAL AS TARGET_MATERIAL,\n",
    "        s.SIMILARITY_SCORE\n",
    "    FROM DATA_SCIENCE.PART_SIMILARITY_SCORES s\n",
    "    JOIN ATOMIC.PART_MASTER p1 ON s.SOURCE_GLOBAL_ID = p1.GLOBAL_ID\n",
    "    JOIN ATOMIC.PART_MASTER p2 ON s.TARGET_GLOBAL_ID = p2.GLOBAL_ID\n",
    "    WHERE s.SIMILARITY_SCORE >= {HIGH_THRESHOLD}\n",
    "    ORDER BY s.SIMILARITY_SCORE DESC\n",
    "    LIMIT 10\n",
    "    \"\"\",\n",
    "    name=\"top_similarity_pairs\"\n",
    ")\n",
    "\n",
    "print(f\"Top 10 Highest-Similarity Pairs (score >= {HIGH_THRESHOLD}):\")\n",
    "print(\"=\" * 80)\n",
    "if len(top_pairs_df) > 0:\n",
    "    for _, row in top_pairs_df.iterrows():\n",
    "        print(f\"\\nScore: {row['SIMILARITY_SCORE']:.2f}\")\n",
    "        print(f\"  Source: {row['SOURCE_PART_NAME']} ({row['SOURCE_MATERIAL']})\")\n",
    "        print(f\"  Target: {row['TARGET_PART_NAME']} ({row['TARGET_MATERIAL']})\")\n",
    "else:\n",
    "    print(f\"No pairs found with similarity >= {HIGH_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "production_output_header"
   },
   "source": [
    "---\n",
    "\n",
    "## 8. Production Output Verification\n",
    "\n",
    "Verify that the output tables were written correctly and contain expected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "verify_output_tables_cell"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OUTPUT VERIFICATION\n",
    "# =============================================================================\n",
    "# Validate that output tables contain expected row counts.\n",
    "\n",
    "# Verify embeddings table\n",
    "embeddings_verify = execute_query(\n",
    "    \"SELECT COUNT(*) AS CNT FROM DATA_SCIENCE.PART_EMBEDDINGS\",\n",
    "    name=\"verify_embeddings_final\"\n",
    ")\n",
    "embeddings_count = int(embeddings_verify['CNT'].iloc[0])\n",
    "\n",
    "# Verify similarity scores table\n",
    "scores_verify = execute_query(\n",
    "    \"SELECT COUNT(*) AS CNT FROM DATA_SCIENCE.PART_SIMILARITY_SCORES\",\n",
    "    name=\"verify_scores_final\"\n",
    ")\n",
    "scores_count = int(scores_verify['CNT'].iloc[0])\n",
    "\n",
    "# Expected: TOP_K scores per embedding\n",
    "expected_scores = embeddings_count * TOP_K\n",
    "\n",
    "print(\"Output Table Verification:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n DATA_SCIENCE.PART_EMBEDDINGS\")\n",
    "print(f\"    Rows: {embeddings_count:,}\")\n",
    "print(f\"    Status: {'PASS' if embeddings_count > 0 else 'FAIL'}\")\n",
    "\n",
    "print(f\"\\n DATA_SCIENCE.PART_SIMILARITY_SCORES\")\n",
    "print(f\"    Rows: {scores_count:,}\")\n",
    "print(f\"    Expected: ~{expected_scores:,} (top-{TOP_K} per part)\")\n",
    "print(f\"    Status: {'PASS' if scores_count > 0 else 'FAIL'}\")\n",
    "\n",
    "if embeddings_count == 0 or scores_count == 0:\n",
    "    raise RuntimeError(\"Output verification failed - tables are empty\")\n",
    "\n",
    "print(\"\\nAll output tables verified successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "key_takeaways_section"
   },
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways & Interpretation Guide\n",
    "\n",
    "### What the Model Learned\n",
    "\n",
    "The E5 embedding model captures **semantic similarity** between parts based on their textual descriptions. Parts with:\n",
    "- Similar names, descriptions, or materials cluster together in embedding space\n",
    "- Different terminology but same meaning (e.g., \"bolt\" vs \"fastener\") may still match\n",
    "- Completely different purposes have low similarity even if they share words\n",
    "\n",
    "### Interpretation Guidelines\n",
    "\n",
    "| Score Range | Interpretation | Recommended Action |\n",
    "|-------------|----------------|--------------------|\n",
    "| 80-100 | **Strong match**: Very similar parts | Investigate for consolidation |\n",
    "| 60-80 | **Moderate match**: Related parts | Consider as substitutes |\n",
    "| 40-60 | **Weak match**: Some overlap | Review before using |\n",
    "| 0-40 | **Poor match**: Likely different | Not recommended as similar |\n",
    "\n",
    "### Limitations & Considerations\n",
    "\n",
    "1. **Text quality dependency**: Poor or missing descriptions produce poor embeddings. Garbage in = garbage out.\n",
    "\n",
    "2. **No dimensional awareness**: The model doesn't understand that \"10mm bolt\" and \"12mm bolt\" are different sizes - they will have high similarity because the text is similar.\n",
    "\n",
    "3. **Sample limitation**: This notebook processes only 5,000 parts. Production deployment should remove or increase the LIMIT clause.\n",
    "\n",
    "4. **Top-K truncation**: Only storing top-3 matches means some useful moderate-similarity pairs are discarded.\n",
    "\n",
    "### Mathematical Recap\n",
    "\n",
    "**Embedding**: $\\mathbf{e} = \\text{E5}(\\text{text}) \\in \\mathbb{R}^{768}$\n",
    "\n",
    "**Cosine Similarity**: $\\text{sim}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{||\\mathbf{a}|| \\cdot ||\\mathbf{b}||}$\n",
    "\n",
    "**Score Scaling**: $\\text{score} = \\text{sim} \\times 100$ (for readability)\n",
    "\n",
    "### Further Learning Resources\n",
    "\n",
    "- [Snowflake Cortex LLM Functions](https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions)\n",
    "- [E5 Embedding Model Paper](https://arxiv.org/abs/2212.03533)\n",
    "- [Understanding Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Increase sample size**: Remove LIMIT for production to process all parts\n",
    "2. **Add metadata filtering**: Pre-filter by category before computing similarity\n",
    "3. **Human validation**: Review high-scoring pairs with domain experts\n",
    "4. **Integrate with procurement**: Connect similarity scores to purchasing workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
